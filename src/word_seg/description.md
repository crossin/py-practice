# 中文分词
### 【问题描述】

中文分词，是自然语言处理中一个很基础性的问题。所谓分词，就是将一整句话拆分成一系列语素，例如：

> 北京/是/中国/首都

显然，我们将主谓宾定语都划分了出来。

而由于中文的特殊性（很多亚洲语言也是一样），分词是一个比较困难的任务。而英文等语言则不存在这个问题，因为英文天然地由空格分隔。

虽然分词是一个非常困难的任务，不过好在这是一个已解决的问题。现代的基于统计的分词器已经可以达到不亚于人的分词效果。而分词没有绝对的好坏，诸如：

> 南京市长江大桥

这种句子，即使是人，也无法确定性地进行分词（万一南京市长真的叫江大桥呢？），不过我们基于统计模型，还是可以得到更贴近于真实情况的分词。

你需要做的是：

* 对所给的文本文件进行分词
* 对所给文本进行分词的同时，标记各个词的词性





### 【[解决思路](solution.md)】

